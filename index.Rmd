--- 
title: "Distribution Theory"
author: "AbdulHafiz Abba"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Distribution Functions, Probability Densities, and Standard Distribution Functions

## Distribution Functions

Distribution functions, also known as cumulative distribution functions (CDFs), are fundamental in understanding the behavior of random variables. A distribution function \( F(x) \) for a random variable \( X \) is defined as:

\[ F(x) = P(X \leq x) \]

This function gives the probability that the random variable \( X \) takes on a value less than or equal to \( x \).

## Probability Densities and their Relations

Probability densities are functions that describe the likelihood of a continuous random variable taking on a particular value. For a continuous random variable \( X \), the probability density function (PDF), denoted as \( f(x) \), is defined such that for any interval \((a, b]\), the probability that \( X \) falls in that interval is given by:

\[ P(a < X \leq b) = \int_{a}^{b} f(x) \, dx \]

The relationship between the probability density function \( f(x) \) and the cumulative distribution function \( F(x) \) is:

\[ F(x) = \int_{-\infty}^{x} f(t) \, dt \]

This means that the CDF is the integral of the PDF from \(-\infty\) to \(x\). 

## Standard Distribution Functions

Standard distribution functions are those that are commonly encountered and have well-defined properties. Some of the most important standard distribution functions include:

1. **Normal Distribution (Gaussian Distribution)**:
   - Characterized by its bell-shaped curve.
   - Described by two parameters: mean (\( \mu \)) and standard deviation (\( \sigma \)).
   - The probability density function of the standard normal distribution (\( N(0,1) \)) is given by:
     \[ f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \]

2. **Uniform Distribution**:
   - All outcomes in a range are equally likely.
   - Defined by its minimum and maximum values.
   - The probability density function for a uniform distribution on the interval \([a, b]\) is:
     \[ f(x) = \frac{1}{b - a} \]

3. **Exponential Distribution**:
   - Models the time between events in a Poisson process.
   - Characterized by a single parameter \( \lambda \), the rate parameter.
   - The probability density function of the exponential distribution is:
     \[ f(x) = \lambda e^{-\lambda x} \]

These standard distribution functions play crucial roles in various statistical analyses and are foundational in probability theory and statistical inference.


## Discrete Random Variables

In many real-world scenarios, we encounter random experiments and variables whose outcomes can be analyzed systematically. This chapter focuses on understanding discrete random variables, which are often found in various applications.

### Example 1: Voice Communication System

Consider a voice communication system with 48 external lines. Let's define a random variable \( X \) representing the number of lines in use at a given time. \( X \) can take integer values from 0 to 48. For instance, if 10 lines are in use, \( x = 10 \).


### Example 2: Customer Arrivals at a Store

Consider a retail store where customers arrive randomly throughout the day. Let \( X \) represent the number of customers arriving in the next hour. \( X \) can take non-negative integer values, indicating the count of arrivals. For instance, \( X = 5 \) means that five customers arrive in the next hour.

### Example 3: Defective Products in Manufacturing

In a manufacturing facility, products undergo quality control inspections. Let \( X \) be the number of defective products found in a batch of 100 items. \( X \) can range from 0 to 100, representing the count of defective items. For example, if \( X = 10 \), it means that 10 out of 100 items are defective.

### Example 4: Number of Accidents on a Highway

Consider a stretch of highway where accidents occur randomly over time. Let \( X \) denote the number of accidents that happen in a day. \( X \) is a discrete random variable, taking non-negative integer values. If \( X = 2 \), it indicates that two accidents occurred during the day.

### Example 5: Flaws in a Software Program

In software development, defects or flaws are inevitable. Let \( X \) represent the number of bugs found in a new software release during the first week of testing. \( X \) can vary from 0 to a potentially large number, indicating the count of defects discovered. For instance, \( X \)


## Understanding Probability Distributions and Probability Mass Functions

Probability distributions and probability mass functions (PMFs) are concepts used to describe the likelihood of different outcomes in a random process. Let's break down these ideas in simpler terms:

### What is a Probability Distribution?

Imagine you're playing a game with a spinner that has different colored sections. Each time you spin the spinner, it lands on a different color. A probability distribution tells you the chances of landing on each color. For example, it might say there's a 30% chance of landing on blue, a 20% chance of landing on red, and so on.

### What is a Probability Mass Function (PMF)?

A probability mass function is like a detailed map of the probabilities in our game with the spinner. It shows us the probability of every possible outcome. So, if our spinner has colors blue, red, green, and yellow, the PMF would tell us the chances of landing on each of these colors. For instance, it might say there's a 30% chance of landing on blue, a 25% chance of landing on red, a 20% chance of landing on green, and a 25% chance of landing on yellow.

### How Do They Help?

Understanding probability distributions and PMFs helps us make predictions and decisions in uncertain situations. By knowing the probabilities of different outcomes, we can anticipate what might happen and plan accordingly. For example, if we know the probability distribution of student grades in a class, we can estimate how many students might get an A, B, or C, which can help teachers prepare their lessons.


## Probability Mass Function (PMF) for Discrete Random Variables

For a discrete random variable \( X \) with possible values \( x_1, x_2, . . . x_n \), a **probability mass function (PMF)** is a function such that:

1. \(
f(x_i) \geq 0 \quad \text{for all} \quad x_i
\): It assigns a probability \( f(x_i) \) to each possible value \( x_i \).
2. \(
f(x_i) = P(X=x_i)
\): The probability assigned to each value \( x_i \) is non-negative.
3. \(
\sum_{i}^{n} f(x_i) = 1
\): The sum of probabilities for all possible values \( x_i \) equals 1, denoted as:

### Example: Probability Distribution of Admissions at State University Gadau

Let's consider State University Gadau as a case study to determine the probability distribution of admissions. We define a random variable \( X \) to represent the number of applicants admitted to the university. Assume that the probability of admission for each applicant is 0.3, and admissions are independent of each other. Our goal is to determine the probability distribution of \( X \).

#### Special Cases:
- If the first applicant is admitted, then \( X = 1 \).
- If the first applicant is rejected but the second is admitted, then \( X = 2 \).
- This pattern continues for subsequent applicants.

#### Probability Distribution:
The probability of admitting the \( k \)th applicant is \( (1 - 0.3)^{k - 1} \times 0.3 \).
- We use the formula \( P(X = k) = (1 - p)^{k - 1} \times p \) to describe the probabilities associated with \( X \).
- This formula represents a geometric random variable, where \( p \) is the probability of success (admission) in each trial.

#### Calculation:
Let's calculate the probability of admission for the first few applicants:
- For \( X = 1 \): \( P(X = 1) = (1 - 0.3)^{0} \times 0.3 = 0.3 \).
- For \( X = 2 \): \( P(X = 2) = (1 - 0.3)^{1} \times 0.3 = 0.21 \).
- Similarly, we can calculate for \( X = 3, 4, \ldots \).

### Example 3-13: Determining the Probability Mass Function of a Random Variable

The sample space of a random experiment is \( S = \{a, b, c, d, e, f\} \),and each outcome is equally likely. A random variable is defined as follows:

| Outcome | \(x\)   |
|---------|--------|
| a       |   0    |
| b       |   0    |
| c       |  1.5   |
| d       |  1.5   |
| e       |   2    |
| f       |   3    |

Determine the probability mass function (PMF) of \( X \). 

**Solution**
We need to find the probability of each value in the range of \( X \).

#### Calculation:
- \( P(X = 0) \): The values 0 occur twice in the outcomes, for outcomes a and b. Since each outcome is equally likely, \( P(X = 0) = \frac{2}{6} = \frac{1}{3} \).
- \( P(X = 1.5) \): The values 1.5 occur twice in the outcomes, for outcomes c and d. Hence, \( P(X = 1.5) = \frac{2}{6} = \frac{1}{3} \).
- \( P(X = 2) \): The value 2 occurs once in the outcomes, for outcome e. Thus, \( P(X = 2) = \frac{1}{6} \).
- \( P(X = 3) \): The value 3 occurs once in the outcomes, for outcome f. Therefore, \( P(X = 3) = \frac{1}{6} \).

#### Probability Mass Function (PMF) of \( X \):
- \( P(X = 0) = \frac{1}{3} \)
- \( P(X = 1.5) = \frac{1}{3} \)
- \( P(X = 2) = \frac{1}{6} \)
- \( P(X = 3) = \frac{1}{6} \)

The probability mass function of \( X \) is given by:
\[ P(X = x) = \begin{cases} 
\frac{1}{3} & \text{if } x = 0 \text{ or } x = 1.5 \\
\frac{1}{6} & \text{if } x = 2 \text{ or } x = 3 \\
0 & \text{otherwise} 
\end{cases} \]

This PMF describes the probabilities associated with each possible value of the random variable \( X \).


### Example 3-25

An assembly consists of three mechanical components. The probabilities that the first, second, and third components meet specifications are 0.95, 0.98, and 0.99, respectively. Assume that the components are independent. Determine the probability mass function of the number of components in the assembly that meet specifications.

#### Solution:

Let's define a random variable \( X \) to represent the number of components in the assembly that meet specifications. We can observe that \( X \) can take on values from 0 to 3, as there can be zero, one, two, or all three components that meet specifications.

#### Calculation:
- \( P(X = 0) \): The probability that none of the components meet specifications is calculated by multiplying the probabilities of each component failing to meet specifications: \( P(X = 0) = (1 - 0.95) \times (1 - 0.98) \times (1 - 0.99) \).
- \( P(X = 1) \): The probability that exactly one component meets specifications is calculated by considering the probability that one component meets specifications and the other two fail: \( P(X = 1) = (0.95) \times (1 - 0.98) \times (1 - 0.99) \), and so on for \( X = 2 \) and \( X = 3 \).

#### Probability Mass Function (PMF) of \( X \):
- \( P(X = 0) = (1 - 0.95) \times (1 - 0.98) \times (1 - 0.99) = 8 \times 10^{-6}\)
- \( P(X = 1) = (0.95) \times (1 - 0.98) \times (1 - 0.99) + (1 - 0.95) \times (0.98) \times (1 - 0.99) + (1 - 0.95) \times (1 - 0.98) \times (0.99) = 0.0012\)
- \( P(X = 2) = (0.95) \times (0.98) \times (1 - 0.99) + (0.95) \times (1 - 0.98) \times (0.99) + (1 - 0.95) \times (0.98) \times (0.99)  = 0.0576 \)
- \( P(X = 3) = (0.95) \times (0.98) \times (0.99)  = 0.9412\)

Calculating these probabilities will give us the probability mass function of \( X \), representing the likelihood of having 0, 1, 2, or 3 components in the assembly that meet specifications.

## Cumulative Distribution Functions (CDFs)

In probability theory and statistics, a Cumulative Distribution Function (CDF) is a function that gives the probability that a random variable \( X \) will take on a value less than or equal to a given value \( x \). Mathematically, the CDF of a random variable \( X \), denoted by \( F(x) \), is defined as:

\[ F(x) = P(X \leq x) \]

In simpler terms, the CDF at a specific value \( x \) tells us the probability that the random variable \( X \) will be less than or equal to \( x \). This function provides a comprehensive summary of the distribution of \( X \), capturing both the probability of individual values and cumulative probabilities up to a certain value.

### Properties of CDFs:

The cumulative distribution function of a discrete random variable X, denoted as $F(x)$, is
\[ F(x) = P(X \le x)=\sum_{x_i \le x}f(x_i) \]

For a discrete random variable X, $F(x)$ satisfies the following properties.

1. **Non-Decreasing**: \(F(x) = P(X \le x)=\sum_{x_i \le x}f(x_i) \) meaning as \( x \) increases, \( F(x) \) does not decrease.

2. **Bounded**: \(0 \le F(x) \le 1\) The CDF is bounded between 0 and 1, inclusive.

3. **Monotonicity**: If \( x \le y \), then \( F(x) \le F(y) \). In other words, the CDF is monotonically non-decreasing.

### Example:
Consider a random variable \( X \) representing the outcome of rolling a fair six-sided die. The CDF of \( X \) would be:

\[ F(x) = \begin{cases} 
0 & \text{if } x < 1 \\
\frac{1}{6} & \text{if } 1 \leq x < 2 \\
\frac{2}{6} & \text{if } 2 \leq x < 3 \\
\frac{3}{6} & \text{if } 3 \leq x < 4 \\
\frac{4}{6} & \text{if } 4 \leq x < 5 \\
\frac{5}{6} & \text{if } 5 \leq x < 6 \\
1 & \text{if } x \geq 6 \\
\end{cases} \]

This CDF tells us the probability of rolling a number less than or equal to \( x \) on a fair six-sided die. For example, \( F(3) \) would be \( \frac{3}{6} = 0.5 \), indicating a 50% chance of rolling a number less than or equal to 3.


